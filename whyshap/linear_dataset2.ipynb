{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib.pyplot import subplots\n",
    "data = pd.read_pickle('/home/jinzhuo/ww_dataset/SCMS/aplysia_ganglia/Aplysia_Data.pkl')\n",
    "sample_info = pd.read_pickle('/home/jinzhuo/ww_dataset/SCMS/aplysia_ganglia/Aplysia_SampleInfo.pkl')\n",
    "feature_info = pd.read_pickle('/home/jinzhuo/ww_dataset/SCMS/aplysia_ganglia/Aplysia_Features.pkl')\n",
    "\n",
    "data['type'] = sample_info['types']\n",
    "types = list({t for t in data['type']})\n",
    "types = sorted(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 542)\n",
      "(2000, 542) Abdom\n",
      "(2000, 542)\n",
      "(2000, 542) Bag\n",
      "(2000, 542)\n",
      "(2000, 542) Buccal\n",
      "(2000, 542)\n",
      "(2000, 542) Cere\n",
      "(2000, 542)\n",
      "(2000, 542) Pedal\n",
      "(2000, 542)\n",
      "(2000, 542) Pleural\n"
     ]
    }
   ],
   "source": [
    "data_matrix_list_train = []\n",
    "data_matrix_list_val = []\n",
    "mean = []\n",
    "ratio_max = []\n",
    "for i in range(len(types)):\n",
    "    array_temp = np.array(data[data['type']==types[i]].drop(['type'],axis  = 1))[:,:]\n",
    "    # 计算每一行的和\n",
    "    row_sums = np.sum(array_temp, axis=1)\n",
    "\n",
    "    # 获取按照行和排序的索引\n",
    "    sorted_indices = np.argsort(row_sums)\n",
    "\n",
    "    # 根据排序索引对原始数组进行排序\n",
    "    array_temp = array_temp[sorted_indices]\n",
    "\n",
    "    array_temp = array_temp[:2000]\n",
    "    np.random.shuffle(array_temp)\n",
    "    np.random.shuffle(array_temp)\n",
    "    data_matrix_list_train.append(array_temp)\n",
    "    # data_matrix_list_val.append(array_temp[:500])\n",
    "    print(array_temp.shape)\n",
    "    mean.append(np.mean(array_temp, axis = 0))\n",
    "    print(data_matrix_list_train[i].shape,types[i])\n",
    "    ratio_max.append(data_matrix_list_train[i].shape[0])\n",
    "cell_type_ref  = np.array(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MixDataset(data_matrix_list,noise_percentage,ratio):\n",
    "    cell_type_num = len(data_matrix_list)\n",
    "    data_matrix_list_noise = [None]*cell_type_num \n",
    "    for i in range(cell_type_num):\n",
    "        noise = np.random.uniform(-0.1, 0.1, data_matrix_list[i].shape)\n",
    "        data_matrix_list_noise[i] = data_matrix_list[i] + noise_percentage*noise*data_matrix_list[i]\n",
    "    \n",
    "    mix_sample_spec = np.empty((cell_type_num,data_matrix_list_noise[0].shape[-1]))\n",
    "    for i in range(cell_type_num):\n",
    "        if ratio[i] == 0:\n",
    "            mix_sample_spec[i] = np.zeros(data_matrix_list_noise[0].shape[-1])\n",
    "        else:\n",
    "            random_indices = np.random.choice(data_matrix_list_noise[i].shape[0], ratio[i], replace=False)\n",
    "            random_samples = data_matrix_list_noise[i][random_indices]\n",
    "            mix_sample_spec[i] = np.sum(random_samples, axis=0)\n",
    "    mix_sample_spec = np.sum(mix_sample_spec, axis=0)\n",
    "    mix_sample_spec = mix_sample_spec + np.random.uniform(-0.1, 0.1, mix_sample_spec.shape)*noise_percentage*mix_sample_spec\n",
    "    return mix_sample_spec\n",
    "def norm_method(method,pretensor):\n",
    "    if method == 'Min_Max_Scaling':\n",
    "        min_vals, _ = torch.min(pretensor, dim=1, keepdim=True)\n",
    "        max_vals, _ = torch.max(pretensor, dim=1, keepdim=True)\n",
    "        posttensor = (pretensor - min_vals) / (max_vals - min_vals)\n",
    "        # return posttensor\n",
    "    elif method == 'z_score':\n",
    "        mean = pretensor.mean()\n",
    "        std = pretensor.std()\n",
    "        # 对Tensor进行Z-Score归一化\n",
    "        posttensor = (pretensor - mean) / std\n",
    "    \n",
    "    elif method == 'l2_norm':\n",
    "        l2_norm = torch.norm(pretensor, p=2,dim = 1)\n",
    "        # 进行L2范数归一化\n",
    "        posttensor = pretensor / l2_norm[:,None]\n",
    "    \n",
    "    elif method == 'Max_Scaling':\n",
    "        # min_vals, _ = torch.min(pretensor, dim=1, keepdim=True)\n",
    "        max_vals, _ = torch.max(pretensor, dim=1, keepdim=True)\n",
    "        posttensor = (pretensor) / (max_vals)\n",
    "    return posttensor\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,cell_type_num,init_choice):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(1, cell_type_num))  # 定义一个10x10的可学习矩阵\n",
    "        if init_choice == True:\n",
    "            init.constant_(self.weight, 1/cell_type_num)\n",
    "    def forward(self, x,norm_method_name):\n",
    "        output = torch.mm(self.weight/torch.sum(self.weight),x)  # 使用可学习的矩阵进行矩阵乘法运算\n",
    "        output = norm_method(norm_method_name,output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 定义一个函数来裁剪权重\n",
    "def clip_weights(model, min_clip=0.0, max_clip=1.0):\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = torch.clamp(param.data, min_clip, max_clip)\n",
    "\n",
    "\n",
    "def linear_deconvolution(cell_type_num,target,cell_ref,init_choice,clip_choice,lossfunc_name,norm_method_name):\n",
    "    model = []\n",
    "    model = MyModel(cell_type_num,init_choice)\n",
    "    if lossfunc_name == 'mae':\n",
    "        lossfunc = nn.L1Loss()\n",
    "    elif lossfunc_name == 'mse':\n",
    "        lossfunc = nn.MSELoss()\n",
    "    elif lossfunc_name == 'smooth':\n",
    "        lossfunc = nn.SmoothL1Loss()\n",
    "    # model.to(device)\n",
    "    LEARNING_RATE = 0.001\n",
    "    optimizer = []\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_step = 1000\n",
    "    # cell_ref = cell_ref.to(device)\n",
    "    # target = target.to(device)\n",
    "    for i in range(train_step):\n",
    "        output = model(cell_ref,norm_method_name)\n",
    "        # loss = lossmse(output,target)\n",
    "        loss = lossfunc(output,target)\n",
    "        # loss = losssmooth(output,target)\n",
    "        # print(\"loss\",loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if clip_choice == True:\n",
    "            clip_weights(model, min_clip=0, max_clip=1.0)\n",
    "    return model.weight/torch.sum(model.weight)\n",
    "\n",
    "\n",
    "# import random\n",
    "def ratio_num_generate(totalcellnum,celltypechoice,totalcelltype):\n",
    "    ratio = [0]*totalcelltype\n",
    "    norm_ratio = [0]*len(celltypechoice)\n",
    "    random_integers = []\n",
    "    # 生成三个随机整数\n",
    "    for _ in range(len(celltypechoice)-1):\n",
    "        random_integer = np.random.randint(1, totalcellnum - sum(random_integers) - 2)\n",
    "        random_integers.append(random_integer)\n",
    "\n",
    "    # 计算第四个整数以确保和为N\n",
    "    last_integer = totalcellnum - sum(random_integers)\n",
    "    random_integers.append(last_integer)\n",
    "\n",
    "    np.random.shuffle(random_integers)\n",
    "\n",
    "    for i in range(len(random_integers)):\n",
    "        ratio[celltypechoice[i]] = random_integers[i]\n",
    "        norm_ratio[i] = random_integers[i]/sum(random_integers)\n",
    "\n",
    "    \n",
    "    return ratio,norm_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "data = pd.read_pickle('/home/jinzhuo/ww_dataset/SCCML/data/SIMS.pkl')\n",
    "\n",
    "types = list({t for t in data['type']})\n",
    "types = sorted(types)\n",
    "types\n",
    "\n",
    "import joblib\n",
    "# joblib.dump(model, '/home/jinzhuo/ww_dataset/DECOMPOSITION/model_save_file/xgboost_model_SIMS.pkl')\n",
    "model = joblib.load('/home/jinzhuo/ww_dataset/DECOMPOSITION/model_save_file/xgboost_model_SIMS.pkl')\n",
    "import shap\n",
    "from shap import TreeExplainer\n",
    "shap_explainer = TreeExplainer(model)\n",
    "shap_values = shap_explainer.shap_values(data.drop(['type'],axis =1))\n",
    "# shap.summary_plot(shap_values, data.drop(['type'],axis =1), plot_type=\"bar\")\n",
    "\n",
    "\n",
    "shap_values = shap_values.reshape(1,1542,4667)\n",
    "\n",
    "\n",
    "features = data.columns[1:]\n",
    "\n",
    "### m/z index select from shap_value ###\n",
    "\n",
    "shap_index = []\n",
    "for i in range(1):\n",
    "    contrib_xgb_best = shap_values[i]\n",
    "    shap_ranked_index = np.argsort(abs(contrib_xgb_best).mean(0))[::-1]\n",
    "    shap_index.append(shap_ranked_index)\n",
    "shap_index = np.array(shap_index)\n",
    "shap_index = shap_index[:,:20]\n",
    "shap_index = shap_index.reshape(-1)\n",
    "shap_index = list(set(list(shap_index)))\n",
    "print(len(shap_index))\n",
    "\n",
    "### m/z index select from top intensity ###\n",
    "mean = []\n",
    "for i in range(2):\n",
    "    print(i,data[data['type']==types[i]].shape,types[i])\n",
    "    mean.append(list(data[data['type']==types[i]].drop(['type'],axis  = 1).mean()))\n",
    "ref_map = np.array(mean)\n",
    "n_largest = 20\n",
    "indices = np.argsort(ref_map, axis=1)[:, -n_largest:]\n",
    "indices = list(indices.reshape(-1))\n",
    "unique_list = sorted([x for i, x in enumerate(indices) if x not in indices[:i]])\n",
    "top_index = unique_list\n",
    "print(len(top_index))\n",
    "\n",
    "### m/z index select from combine both ###\n",
    "combine_index = list(set(shap_index+top_index))\n",
    "print(len(combine_index))\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "def MixDataset(data_matrix_list,noise_percentage,ratio):\n",
    "    cell_type_num = len(data_matrix_list)\n",
    "    data_matrix_list_noise = [None]*cell_type_num \n",
    "    for i in range(cell_type_num):\n",
    "        noise = np.random.uniform(-0.1, 0.1, data_matrix_list[i].shape)\n",
    "        data_matrix_list_noise[i] = data_matrix_list[i] + noise_percentage*noise*data_matrix_list[i]\n",
    "    \n",
    "    mix_sample_spec = np.empty((cell_type_num,data_matrix_list_noise[0].shape[-1]))\n",
    "    for i in range(cell_type_num):\n",
    "        if ratio[i] == 0:\n",
    "            mix_sample_spec[i] = np.zeros(data_matrix_list_noise[0].shape[-1])\n",
    "        else:\n",
    "            random_indices = np.random.choice(data_matrix_list_noise[i].shape[0], ratio[i], replace=False)\n",
    "            random_samples = data_matrix_list_noise[i][random_indices]\n",
    "            mix_sample_spec[i] = np.sum(random_samples, axis=0)\n",
    "    mix_sample_spec = np.sum(mix_sample_spec, axis=0)\n",
    "    mix_sample_spec = mix_sample_spec + np.random.uniform(-0.1, 0.1, mix_sample_spec.shape)*noise_percentage*mix_sample_spec\n",
    "    return mix_sample_spec\n",
    "    \n",
    "\n",
    "data_matrix_list_train = []\n",
    "data_matrix_list_val = []\n",
    "mean = []\n",
    "ratio_max = []\n",
    "for i in range(len(types)):\n",
    "    array_temp = np.array(data[data['type']==types[i]].drop(['type'],axis  = 1))[:,shap_index]\n",
    "    # 计算每一行的和\n",
    "    row_sums = np.sum(array_temp, axis=1)\n",
    "\n",
    "    # 获取按照行和排序的索引\n",
    "    sorted_indices = np.argsort(row_sums)\n",
    "\n",
    "    # 根据排序索引对原始数组进行排序\n",
    "    array_temp = array_temp[sorted_indices]\n",
    "\n",
    "    array_temp = array_temp[:500]\n",
    "    np.random.shuffle(array_temp)\n",
    "    np.random.shuffle(array_temp)\n",
    "    data_matrix_list_train.append(array_temp[:500])\n",
    "    data_matrix_list_val.append(array_temp[:500])\n",
    "    mean.append(np.mean(array_temp[:1000], axis = 0))\n",
    "    print(data_matrix_list_train[i].shape,types[i])\n",
    "    ratio_max.append(data_matrix_list_train[i].shape[0])\n",
    "cell_type_ref  = np.array(mean)\n",
    "\n",
    "# cell_type_ref = ref_map[:,combine_index]\n",
    "\n",
    "def norm_method(method,pretensor):\n",
    "    if method == 'Min_Max_Scaling':\n",
    "        min_vals, _ = torch.min(pretensor, dim=1, keepdim=True)\n",
    "        max_vals, _ = torch.max(pretensor, dim=1, keepdim=True)\n",
    "        posttensor = (pretensor - min_vals) / (max_vals - min_vals)\n",
    "        # return posttensor\n",
    "    elif method == 'z_score':\n",
    "        mean = pretensor.mean()\n",
    "        std = pretensor.std()\n",
    "        # 对Tensor进行Z-Score归一化\n",
    "        posttensor = (pretensor - mean) / std\n",
    "    \n",
    "    elif method == 'l2_norm':\n",
    "        l2_norm = torch.norm(pretensor, p=2,dim = 1)\n",
    "        # 进行L2范数归一化\n",
    "        posttensor = pretensor / l2_norm[:,None]\n",
    "    \n",
    "    elif method == 'Max_Scaling':\n",
    "        # min_vals, _ = torch.min(pretensor, dim=1, keepdim=True)\n",
    "        max_vals, _ = torch.max(pretensor, dim=1, keepdim=True)\n",
    "        posttensor = (pretensor) / (max_vals)\n",
    "    return posttensor\n",
    "\n",
    "\n",
    "# def custom_sigmoid(x):\n",
    "#     return -1 / (1 + torch.exp(-500 * (x - 0.001)))+1\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,cell_type_num,init_choice):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(1, cell_type_num))  # 定义一个10x10的可学习矩阵\n",
    "        if init_choice == True:\n",
    "            init.constant_(self.weight, 1/cell_type_num)\n",
    "    def forward(self, x,norm_method_name):\n",
    "        output = torch.mm(self.weight/torch.sum(self.weight),x)  # 使用可学习的矩阵进行矩阵乘法运算\n",
    "        output = norm_method(norm_method_name,output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 定义一个函数来裁剪权重\n",
    "def clip_weights(model, min_clip=0.0, max_clip=1.0):\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = torch.clamp(param.data, min_clip, max_clip)\n",
    "\n",
    "\n",
    "def linear_deconvolution(cell_type_num,target,cell_ref,init_choice,clip_choice,lossfunc_name,norm_method_name):\n",
    "    model = []\n",
    "    model = MyModel(cell_type_num,init_choice)\n",
    "    if lossfunc_name == 'mae':\n",
    "        lossfunc = nn.L1Loss()\n",
    "    elif lossfunc_name == 'mse':\n",
    "        lossfunc = nn.MSELoss()\n",
    "    elif lossfunc_name == 'smooth':\n",
    "        lossfunc = nn.SmoothL1Loss()\n",
    "    # model.to(device)\n",
    "    LEARNING_RATE = 0.001\n",
    "    optimizer = []\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_step = 1000\n",
    "    # cell_ref = cell_ref.to(device)\n",
    "    # target = target.to(device)\n",
    "    for i in range(train_step):\n",
    "        output = model(cell_ref,norm_method_name)\n",
    "        # loss = lossmse(output,target)\n",
    "        loss = lossfunc(output,target)\n",
    "        # loss = losssmooth(output,target)\n",
    "        # print(\"loss\",loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if clip_choice == True:\n",
    "            clip_weights(model, min_clip=0, max_clip=1.0)\n",
    "    return model.weight/torch.sum(model.weight)\n",
    "\n",
    "\n",
    "# import random\n",
    "def ratio_num_generate(totalcellnum,celltypechoice,totalcelltype):\n",
    "    ratio = [0]*totalcelltype\n",
    "    norm_ratio = [0]*len(celltypechoice)\n",
    "    random_integers = []\n",
    "    # 生成三个随机整数\n",
    "    for _ in range(len(celltypechoice)-1):\n",
    "        random_integer = np.random.randint(1, totalcellnum - sum(random_integers) - 2)\n",
    "        random_integers.append(random_integer)\n",
    "\n",
    "    # 计算第四个整数以确保和为N\n",
    "    last_integer = totalcellnum - sum(random_integers)\n",
    "    random_integers.append(last_integer)\n",
    "\n",
    "    np.random.shuffle(random_integers)\n",
    "\n",
    "    for i in range(len(random_integers)):\n",
    "        ratio[celltypechoice[i]] = random_integers[i]\n",
    "        norm_ratio[i] = random_integers[i]/sum(random_integers)\n",
    "\n",
    "    \n",
    "    return ratio,norm_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# def split_data(data,test_size,random_state):\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data.drop('type',axis = 1).values, data['type'].values, \n",
    "#                                                         test_size=test_size,\n",
    "#                                                         random_state=random_state)\n",
    "#     print(y_train)\n",
    "#     y_train = le.fit_transform(y_train)\n",
    "#     print(y_train)\n",
    "#     y_test = le.fit_transform(y_test)\n",
    "\n",
    "#     data_dict = {'X_train':X_train,'X_test':X_test,'y_train':y_train,'y_test':y_test}\n",
    "\n",
    "#     return data_dict\n",
    "\n",
    "# data_dict_icc = split_data(data = data,test_size=0.2,random_state=11)\n",
    "\n",
    "# import xgboost\n",
    "# from sklearn.metrics import classification_report \n",
    "# model = xgboost.XGBClassifier(n_estimators=600,subsample=0.8)\n",
    "# def train_model(model,data_dict):\n",
    "    \n",
    "#     model.fit(data_dict['X_train'],data_dict['y_train'])\n",
    "\n",
    "#     y_pred = model.predict(data_dict['X_test'])\n",
    "#     prob = model.predict_proba(data_dict['X_test'])\n",
    "#     report_dict = classification_report(data_dict['y_test'], y_pred, output_dict=True)\n",
    "    \n",
    "#     return model, y_pred, prob,report_dict\n",
    "\n",
    "# model,y_pred,prob,report_dict = train_model(model,data_dict_icc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(init_choice ,clip_choice ,lossfunc_name,sample_list,celltypechoice,norm_method_name):\n",
    "    for j in range(200):\n",
    "        ratio,norm_ratio = ratio_num_generate(50*2,celltypechoice,6)\n",
    "        print(ratio,norm_ratio)\n",
    "        # MixSample = MixDataset(data_matrix_list_train,ratio)\n",
    "        MixSample = MixDataset(data_matrix_list_train,3,ratio)\n",
    "        target = MixSample.reshape(1,-1)\n",
    "        target = torch.from_numpy(target).float()\n",
    "        target = norm_method(norm_method_name,target)\n",
    "        predict = linear_deconvolution(2,target,torch.from_numpy(cell_type_ref[celltypechoice,:]).float(),init_choice = init_choice,clip_choice=clip_choice,lossfunc_name = lossfunc_name,norm_method_name = norm_method_name )\n",
    "        \n",
    "        print(lossfunc_name,j,predict,norm_ratio,ratio)\n",
    "        sample_list.append([predict,norm_ratio,ratio])\n",
    "    return sample_list\n",
    "celltypechoice = [0,1]\n",
    "\n",
    "# sample_listmae = []\n",
    "# sample_listmse = []\n",
    "# sample_listsmooth = []\n",
    "# sample_list_withoutinit = []\n",
    "# sample_list_withoutclip = []\n",
    "# sample_list_withoutinitandclip = []\n",
    "# sample_listmae = train(init_choice = True ,clip_choice=True ,lossfunc_name='mae',sample_list=sample_listmae,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "# sample_listmse = train(init_choice = True ,clip_choice=True ,lossfunc_name='mse',sample_list=sample_listmse,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "# sample_listsmooth = train(init_choice = True ,clip_choice=True ,lossfunc_name='smooth',sample_list=sample_listsmooth,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "# sample_list_withoutclip = train(init_choice = True ,clip_choice=False ,lossfunc_name='mse',sample_list=sample_list_withoutclip,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "# sample_list_withoutinit = train(init_choice = False ,clip_choice=True ,lossfunc_name='mse',sample_list=sample_list_withoutinit,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "# sample_list_withoutinitandclip = train(init_choice = False ,clip_choice=False ,lossfunc_name='mse',sample_list=sample_list_withoutinitandclip,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 97, 0, 0, 0, 0] [0.03, 0.97]\n",
      "mse 0 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.03, 0.97] [3, 97, 0, 0, 0, 0]\n",
      "[31, 69, 0, 0, 0, 0] [0.31, 0.69]\n",
      "mse 1 tensor([[0.2735, 0.7265]], grad_fn=<DivBackward0>) [0.31, 0.69] [31, 69, 0, 0, 0, 0]\n",
      "[95, 5, 0, 0, 0, 0] [0.95, 0.05]\n",
      "mse 2 tensor([[0.9176, 0.0824]], grad_fn=<DivBackward0>) [0.95, 0.05] [95, 5, 0, 0, 0, 0]\n",
      "[72, 28, 0, 0, 0, 0] [0.72, 0.28]\n",
      "mse 3 tensor([[0.8471, 0.1529]], grad_fn=<DivBackward0>) [0.72, 0.28] [72, 28, 0, 0, 0, 0]\n",
      "[39, 61, 0, 0, 0, 0] [0.39, 0.61]\n",
      "mse 4 tensor([[0.2985, 0.7015]], grad_fn=<DivBackward0>) [0.39, 0.61] [39, 61, 0, 0, 0, 0]\n",
      "[83, 17, 0, 0, 0, 0] [0.83, 0.17]\n",
      "mse 5 tensor([[0.8169, 0.1831]], grad_fn=<DivBackward0>) [0.83, 0.17] [83, 17, 0, 0, 0, 0]\n",
      "[6, 94, 0, 0, 0, 0] [0.06, 0.94]\n",
      "mse 6 tensor([[0.4574, 0.5426]], grad_fn=<DivBackward0>) [0.06, 0.94] [6, 94, 0, 0, 0, 0]\n",
      "[49, 51, 0, 0, 0, 0] [0.49, 0.51]\n",
      "mse 7 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.49, 0.51] [49, 51, 0, 0, 0, 0]\n",
      "[96, 4, 0, 0, 0, 0] [0.96, 0.04]\n",
      "mse 8 tensor([[0.9235, 0.0765]], grad_fn=<DivBackward0>) [0.96, 0.04] [96, 4, 0, 0, 0, 0]\n",
      "[84, 16, 0, 0, 0, 0] [0.84, 0.16]\n",
      "mse 9 tensor([[0.8558, 0.1442]], grad_fn=<DivBackward0>) [0.84, 0.16] [84, 16, 0, 0, 0, 0]\n",
      "[76, 24, 0, 0, 0, 0] [0.76, 0.24]\n",
      "mse 10 tensor([[0.6416, 0.3584]], grad_fn=<DivBackward0>) [0.76, 0.24] [76, 24, 0, 0, 0, 0]\n",
      "[9, 91, 0, 0, 0, 0] [0.09, 0.91]\n",
      "mse 11 tensor([[0.2520, 0.7480]], grad_fn=<DivBackward0>) [0.09, 0.91] [9, 91, 0, 0, 0, 0]\n",
      "[4, 96, 0, 0, 0, 0] [0.04, 0.96]\n",
      "mse 12 tensor([[0.1094, 0.8906]], grad_fn=<DivBackward0>) [0.04, 0.96] [4, 96, 0, 0, 0, 0]\n",
      "[77, 23, 0, 0, 0, 0] [0.77, 0.23]\n",
      "mse 13 tensor([[0.7105, 0.2895]], grad_fn=<DivBackward0>) [0.77, 0.23] [77, 23, 0, 0, 0, 0]\n",
      "[64, 36, 0, 0, 0, 0] [0.64, 0.36]\n",
      "mse 14 tensor([[0.6216, 0.3784]], grad_fn=<DivBackward0>) [0.64, 0.36] [64, 36, 0, 0, 0, 0]\n",
      "[67, 33, 0, 0, 0, 0] [0.67, 0.33]\n",
      "mse 15 tensor([[0.7960, 0.2040]], grad_fn=<DivBackward0>) [0.67, 0.33] [67, 33, 0, 0, 0, 0]\n",
      "[19, 81, 0, 0, 0, 0] [0.19, 0.81]\n",
      "mse 16 tensor([[0.0742, 0.9258]], grad_fn=<DivBackward0>) [0.19, 0.81] [19, 81, 0, 0, 0, 0]\n",
      "[22, 78, 0, 0, 0, 0] [0.22, 0.78]\n",
      "mse 17 tensor([[0.7621, 0.2379]], grad_fn=<DivBackward0>) [0.22, 0.78] [22, 78, 0, 0, 0, 0]\n",
      "[23, 77, 0, 0, 0, 0] [0.23, 0.77]\n",
      "mse 18 tensor([[0.1678, 0.8322]], grad_fn=<DivBackward0>) [0.23, 0.77] [23, 77, 0, 0, 0, 0]\n",
      "[54, 46, 0, 0, 0, 0] [0.54, 0.46]\n",
      "mse 19 tensor([[0.1979, 0.8021]], grad_fn=<DivBackward0>) [0.54, 0.46] [54, 46, 0, 0, 0, 0]\n",
      "[88, 12, 0, 0, 0, 0] [0.88, 0.12]\n",
      "mse 20 tensor([[0.7269, 0.2731]], grad_fn=<DivBackward0>) [0.88, 0.12] [88, 12, 0, 0, 0, 0]\n",
      "[65, 35, 0, 0, 0, 0] [0.65, 0.35]\n",
      "mse 21 tensor([[0.8401, 0.1599]], grad_fn=<DivBackward0>) [0.65, 0.35] [65, 35, 0, 0, 0, 0]\n",
      "[82, 18, 0, 0, 0, 0] [0.82, 0.18]\n",
      "mse 22 tensor([[0.8108, 0.1892]], grad_fn=<DivBackward0>) [0.82, 0.18] [82, 18, 0, 0, 0, 0]\n",
      "[41, 59, 0, 0, 0, 0] [0.41, 0.59]\n",
      "mse 23 tensor([[0.6300, 0.3700]], grad_fn=<DivBackward0>) [0.41, 0.59] [41, 59, 0, 0, 0, 0]\n",
      "[96, 4, 0, 0, 0, 0] [0.96, 0.04]\n",
      "mse 24 tensor([[0.9635, 0.0365]], grad_fn=<DivBackward0>) [0.96, 0.04] [96, 4, 0, 0, 0, 0]\n",
      "[17, 83, 0, 0, 0, 0] [0.17, 0.83]\n",
      "mse 25 tensor([[0.6452, 0.3548]], grad_fn=<DivBackward0>) [0.17, 0.83] [17, 83, 0, 0, 0, 0]\n",
      "[43, 57, 0, 0, 0, 0] [0.43, 0.57]\n",
      "mse 26 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.43, 0.57] [43, 57, 0, 0, 0, 0]\n",
      "[72, 28, 0, 0, 0, 0] [0.72, 0.28]\n",
      "mse 27 tensor([[0.5086, 0.4914]], grad_fn=<DivBackward0>) [0.72, 0.28] [72, 28, 0, 0, 0, 0]\n",
      "[35, 65, 0, 0, 0, 0] [0.35, 0.65]\n",
      "mse 28 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.35, 0.65] [35, 65, 0, 0, 0, 0]\n",
      "[55, 45, 0, 0, 0, 0] [0.55, 0.45]\n",
      "mse 29 tensor([[0.6858, 0.3142]], grad_fn=<DivBackward0>) [0.55, 0.45] [55, 45, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 30 tensor([[0.8534, 0.1466]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[94, 6, 0, 0, 0, 0] [0.94, 0.06]\n",
      "mse 31 tensor([[0.9801, 0.0199]], grad_fn=<DivBackward0>) [0.94, 0.06] [94, 6, 0, 0, 0, 0]\n",
      "[59, 41, 0, 0, 0, 0] [0.59, 0.41]\n",
      "mse 32 tensor([[0.8040, 0.1960]], grad_fn=<DivBackward0>) [0.59, 0.41] [59, 41, 0, 0, 0, 0]\n",
      "[55, 45, 0, 0, 0, 0] [0.55, 0.45]\n",
      "mse 33 tensor([[0.6935, 0.3065]], grad_fn=<DivBackward0>) [0.55, 0.45] [55, 45, 0, 0, 0, 0]\n",
      "[19, 81, 0, 0, 0, 0] [0.19, 0.81]\n",
      "mse 34 tensor([[0.5419, 0.4581]], grad_fn=<DivBackward0>) [0.19, 0.81] [19, 81, 0, 0, 0, 0]\n",
      "[11, 89, 0, 0, 0, 0] [0.11, 0.89]\n",
      "mse 35 tensor([[0.4064, 0.5936]], grad_fn=<DivBackward0>) [0.11, 0.89] [11, 89, 0, 0, 0, 0]\n",
      "[87, 13, 0, 0, 0, 0] [0.87, 0.13]\n",
      "mse 36 tensor([[0.7972, 0.2028]], grad_fn=<DivBackward0>) [0.87, 0.13] [87, 13, 0, 0, 0, 0]\n",
      "[17, 83, 0, 0, 0, 0] [0.17, 0.83]\n",
      "mse 37 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.17, 0.83] [17, 83, 0, 0, 0, 0]\n",
      "[57, 43, 0, 0, 0, 0] [0.57, 0.43]\n",
      "mse 38 tensor([[0.1139, 0.8861]], grad_fn=<DivBackward0>) [0.57, 0.43] [57, 43, 0, 0, 0, 0]\n",
      "[86, 14, 0, 0, 0, 0] [0.86, 0.14]\n",
      "mse 39 tensor([[0.8575, 0.1425]], grad_fn=<DivBackward0>) [0.86, 0.14] [86, 14, 0, 0, 0, 0]\n",
      "[98, 2, 0, 0, 0, 0] [0.98, 0.02]\n",
      "mse 40 tensor([[0.9945, 0.0055]], grad_fn=<DivBackward0>) [0.98, 0.02] [98, 2, 0, 0, 0, 0]\n",
      "[57, 43, 0, 0, 0, 0] [0.57, 0.43]\n",
      "mse 41 tensor([[0.6351, 0.3649]], grad_fn=<DivBackward0>) [0.57, 0.43] [57, 43, 0, 0, 0, 0]\n",
      "[73, 27, 0, 0, 0, 0] [0.73, 0.27]\n",
      "mse 42 tensor([[0.8073, 0.1927]], grad_fn=<DivBackward0>) [0.73, 0.27] [73, 27, 0, 0, 0, 0]\n",
      "[18, 82, 0, 0, 0, 0] [0.18, 0.82]\n",
      "mse 43 tensor([[0.5427, 0.4573]], grad_fn=<DivBackward0>) [0.18, 0.82] [18, 82, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 44 tensor([[0.7810, 0.2190]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[77, 23, 0, 0, 0, 0] [0.77, 0.23]\n",
      "mse 45 tensor([[0.7700, 0.2300]], grad_fn=<DivBackward0>) [0.77, 0.23] [77, 23, 0, 0, 0, 0]\n",
      "[79, 21, 0, 0, 0, 0] [0.79, 0.21]\n",
      "mse 46 tensor([[0.8327, 0.1673]], grad_fn=<DivBackward0>) [0.79, 0.21] [79, 21, 0, 0, 0, 0]\n",
      "[16, 84, 0, 0, 0, 0] [0.16, 0.84]\n",
      "mse 47 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.16, 0.84] [16, 84, 0, 0, 0, 0]\n",
      "[89, 11, 0, 0, 0, 0] [0.89, 0.11]\n",
      "mse 48 tensor([[0.5605, 0.4395]], grad_fn=<DivBackward0>) [0.89, 0.11] [89, 11, 0, 0, 0, 0]\n",
      "[35, 65, 0, 0, 0, 0] [0.35, 0.65]\n",
      "mse 49 tensor([[0.6479, 0.3521]], grad_fn=<DivBackward0>) [0.35, 0.65] [35, 65, 0, 0, 0, 0]\n",
      "[34, 66, 0, 0, 0, 0] [0.34, 0.66]\n",
      "mse 50 tensor([[0.0788, 0.9212]], grad_fn=<DivBackward0>) [0.34, 0.66] [34, 66, 0, 0, 0, 0]\n",
      "[79, 21, 0, 0, 0, 0] [0.79, 0.21]\n",
      "mse 51 tensor([[0.6642, 0.3358]], grad_fn=<DivBackward0>) [0.79, 0.21] [79, 21, 0, 0, 0, 0]\n",
      "[2, 98, 0, 0, 0, 0] [0.02, 0.98]\n",
      "mse 52 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.02, 0.98] [2, 98, 0, 0, 0, 0]\n",
      "[55, 45, 0, 0, 0, 0] [0.55, 0.45]\n",
      "mse 53 tensor([[0.6902, 0.3098]], grad_fn=<DivBackward0>) [0.55, 0.45] [55, 45, 0, 0, 0, 0]\n",
      "[32, 68, 0, 0, 0, 0] [0.32, 0.68]\n",
      "mse 54 tensor([[0.3143, 0.6857]], grad_fn=<DivBackward0>) [0.32, 0.68] [32, 68, 0, 0, 0, 0]\n",
      "[30, 70, 0, 0, 0, 0] [0.3, 0.7]\n",
      "mse 55 tensor([[0.1814, 0.8186]], grad_fn=<DivBackward0>) [0.3, 0.7] [30, 70, 0, 0, 0, 0]\n",
      "[51, 49, 0, 0, 0, 0] [0.51, 0.49]\n",
      "mse 56 tensor([[0.6566, 0.3434]], grad_fn=<DivBackward0>) [0.51, 0.49] [51, 49, 0, 0, 0, 0]\n",
      "[56, 44, 0, 0, 0, 0] [0.56, 0.44]\n",
      "mse 57 tensor([[0.1813, 0.8187]], grad_fn=<DivBackward0>) [0.56, 0.44] [56, 44, 0, 0, 0, 0]\n",
      "[4, 96, 0, 0, 0, 0] [0.04, 0.96]\n",
      "mse 58 tensor([[0.0698, 0.9302]], grad_fn=<DivBackward0>) [0.04, 0.96] [4, 96, 0, 0, 0, 0]\n",
      "[63, 37, 0, 0, 0, 0] [0.63, 0.37]\n",
      "mse 59 tensor([[0.6674, 0.3326]], grad_fn=<DivBackward0>) [0.63, 0.37] [63, 37, 0, 0, 0, 0]\n",
      "[96, 4, 0, 0, 0, 0] [0.96, 0.04]\n",
      "mse 60 tensor([[0.7771, 0.2229]], grad_fn=<DivBackward0>) [0.96, 0.04] [96, 4, 0, 0, 0, 0]\n",
      "[77, 23, 0, 0, 0, 0] [0.77, 0.23]\n",
      "mse 61 tensor([[0.8639, 0.1361]], grad_fn=<DivBackward0>) [0.77, 0.23] [77, 23, 0, 0, 0, 0]\n",
      "[74, 26, 0, 0, 0, 0] [0.74, 0.26]\n",
      "mse 62 tensor([[0.8119, 0.1881]], grad_fn=<DivBackward0>) [0.74, 0.26] [74, 26, 0, 0, 0, 0]\n",
      "[7, 93, 0, 0, 0, 0] [0.07, 0.93]\n",
      "mse 63 tensor([[0.2660, 0.7340]], grad_fn=<DivBackward0>) [0.07, 0.93] [7, 93, 0, 0, 0, 0]\n",
      "[44, 56, 0, 0, 0, 0] [0.44, 0.56]\n",
      "mse 64 tensor([[0.3585, 0.6415]], grad_fn=<DivBackward0>) [0.44, 0.56] [44, 56, 0, 0, 0, 0]\n",
      "[55, 45, 0, 0, 0, 0] [0.55, 0.45]\n",
      "mse 65 tensor([[0.5009, 0.4991]], grad_fn=<DivBackward0>) [0.55, 0.45] [55, 45, 0, 0, 0, 0]\n",
      "[68, 32, 0, 0, 0, 0] [0.68, 0.32]\n",
      "mse 66 tensor([[0.5846, 0.4154]], grad_fn=<DivBackward0>) [0.68, 0.32] [68, 32, 0, 0, 0, 0]\n",
      "[56, 44, 0, 0, 0, 0] [0.56, 0.44]\n",
      "mse 67 tensor([[0.7625, 0.2375]], grad_fn=<DivBackward0>) [0.56, 0.44] [56, 44, 0, 0, 0, 0]\n",
      "[79, 21, 0, 0, 0, 0] [0.79, 0.21]\n",
      "mse 68 tensor([[0.7880, 0.2120]], grad_fn=<DivBackward0>) [0.79, 0.21] [79, 21, 0, 0, 0, 0]\n",
      "[16, 84, 0, 0, 0, 0] [0.16, 0.84]\n",
      "mse 69 tensor([[0.7290, 0.2710]], grad_fn=<DivBackward0>) [0.16, 0.84] [16, 84, 0, 0, 0, 0]\n",
      "[72, 28, 0, 0, 0, 0] [0.72, 0.28]\n",
      "mse 70 tensor([[0.6294, 0.3706]], grad_fn=<DivBackward0>) [0.72, 0.28] [72, 28, 0, 0, 0, 0]\n",
      "[72, 28, 0, 0, 0, 0] [0.72, 0.28]\n",
      "mse 71 tensor([[0.8141, 0.1859]], grad_fn=<DivBackward0>) [0.72, 0.28] [72, 28, 0, 0, 0, 0]\n",
      "[45, 55, 0, 0, 0, 0] [0.45, 0.55]\n",
      "mse 72 tensor([[0.1593, 0.8407]], grad_fn=<DivBackward0>) [0.45, 0.55] [45, 55, 0, 0, 0, 0]\n",
      "[43, 57, 0, 0, 0, 0] [0.43, 0.57]\n",
      "mse 73 tensor([[0.1029, 0.8971]], grad_fn=<DivBackward0>) [0.43, 0.57] [43, 57, 0, 0, 0, 0]\n",
      "[64, 36, 0, 0, 0, 0] [0.64, 0.36]\n",
      "mse 74 tensor([[0.0666, 0.9334]], grad_fn=<DivBackward0>) [0.64, 0.36] [64, 36, 0, 0, 0, 0]\n",
      "[98, 2, 0, 0, 0, 0] [0.98, 0.02]\n",
      "mse 75 tensor([[0.9782, 0.0218]], grad_fn=<DivBackward0>) [0.98, 0.02] [98, 2, 0, 0, 0, 0]\n",
      "[21, 79, 0, 0, 0, 0] [0.21, 0.79]\n",
      "mse 76 tensor([[0.6243, 0.3757]], grad_fn=<DivBackward0>) [0.21, 0.79] [21, 79, 0, 0, 0, 0]\n",
      "[50, 50, 0, 0, 0, 0] [0.5, 0.5]\n",
      "mse 77 tensor([[0.3209, 0.6791]], grad_fn=<DivBackward0>) [0.5, 0.5] [50, 50, 0, 0, 0, 0]\n",
      "[1, 99, 0, 0, 0, 0] [0.01, 0.99]\n",
      "mse 78 tensor([[0.7821, 0.2179]], grad_fn=<DivBackward0>) [0.01, 0.99] [1, 99, 0, 0, 0, 0]\n",
      "[17, 83, 0, 0, 0, 0] [0.17, 0.83]\n",
      "mse 79 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.17, 0.83] [17, 83, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 80 tensor([[0.7815, 0.2185]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[4, 96, 0, 0, 0, 0] [0.04, 0.96]\n",
      "mse 81 tensor([[0.2358, 0.7642]], grad_fn=<DivBackward0>) [0.04, 0.96] [4, 96, 0, 0, 0, 0]\n",
      "[62, 38, 0, 0, 0, 0] [0.62, 0.38]\n",
      "mse 82 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.62, 0.38] [62, 38, 0, 0, 0, 0]\n",
      "[53, 47, 0, 0, 0, 0] [0.53, 0.47]\n",
      "mse 83 tensor([[0.8252, 0.1748]], grad_fn=<DivBackward0>) [0.53, 0.47] [53, 47, 0, 0, 0, 0]\n",
      "[70, 30, 0, 0, 0, 0] [0.7, 0.3]\n",
      "mse 84 tensor([[0.4393, 0.5607]], grad_fn=<DivBackward0>) [0.7, 0.3] [70, 30, 0, 0, 0, 0]\n",
      "[45, 55, 0, 0, 0, 0] [0.45, 0.55]\n",
      "mse 85 tensor([[0.7271, 0.2729]], grad_fn=<DivBackward0>) [0.45, 0.55] [45, 55, 0, 0, 0, 0]\n",
      "[50, 50, 0, 0, 0, 0] [0.5, 0.5]\n",
      "mse 86 tensor([[0.4814, 0.5186]], grad_fn=<DivBackward0>) [0.5, 0.5] [50, 50, 0, 0, 0, 0]\n",
      "[20, 80, 0, 0, 0, 0] [0.2, 0.8]\n",
      "mse 87 tensor([[0.4541, 0.5459]], grad_fn=<DivBackward0>) [0.2, 0.8] [20, 80, 0, 0, 0, 0]\n",
      "[18, 82, 0, 0, 0, 0] [0.18, 0.82]\n",
      "mse 88 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.18, 0.82] [18, 82, 0, 0, 0, 0]\n",
      "[68, 32, 0, 0, 0, 0] [0.68, 0.32]\n",
      "mse 89 tensor([[0.6054, 0.3946]], grad_fn=<DivBackward0>) [0.68, 0.32] [68, 32, 0, 0, 0, 0]\n",
      "[60, 40, 0, 0, 0, 0] [0.6, 0.4]\n",
      "mse 90 tensor([[0.7254, 0.2746]], grad_fn=<DivBackward0>) [0.6, 0.4] [60, 40, 0, 0, 0, 0]\n",
      "[35, 65, 0, 0, 0, 0] [0.35, 0.65]\n",
      "mse 91 tensor([[0.6227, 0.3773]], grad_fn=<DivBackward0>) [0.35, 0.65] [35, 65, 0, 0, 0, 0]\n",
      "[85, 15, 0, 0, 0, 0] [0.85, 0.15]\n",
      "mse 92 tensor([[0.7484, 0.2516]], grad_fn=<DivBackward0>) [0.85, 0.15] [85, 15, 0, 0, 0, 0]\n",
      "[38, 62, 0, 0, 0, 0] [0.38, 0.62]\n",
      "mse 93 tensor([[0.3798, 0.6202]], grad_fn=<DivBackward0>) [0.38, 0.62] [38, 62, 0, 0, 0, 0]\n",
      "[15, 85, 0, 0, 0, 0] [0.15, 0.85]\n",
      "mse 94 tensor([[0.2002, 0.7998]], grad_fn=<DivBackward0>) [0.15, 0.85] [15, 85, 0, 0, 0, 0]\n",
      "[52, 48, 0, 0, 0, 0] [0.52, 0.48]\n",
      "mse 95 tensor([[0.5822, 0.4178]], grad_fn=<DivBackward0>) [0.52, 0.48] [52, 48, 0, 0, 0, 0]\n",
      "[16, 84, 0, 0, 0, 0] [0.16, 0.84]\n",
      "mse 96 tensor([[0.3253, 0.6747]], grad_fn=<DivBackward0>) [0.16, 0.84] [16, 84, 0, 0, 0, 0]\n",
      "[6, 94, 0, 0, 0, 0] [0.06, 0.94]\n",
      "mse 97 tensor([[0.5107, 0.4893]], grad_fn=<DivBackward0>) [0.06, 0.94] [6, 94, 0, 0, 0, 0]\n",
      "[67, 33, 0, 0, 0, 0] [0.67, 0.33]\n",
      "mse 98 tensor([[0.7518, 0.2482]], grad_fn=<DivBackward0>) [0.67, 0.33] [67, 33, 0, 0, 0, 0]\n",
      "[23, 77, 0, 0, 0, 0] [0.23, 0.77]\n",
      "mse 99 tensor([[0.7377, 0.2623]], grad_fn=<DivBackward0>) [0.23, 0.77] [23, 77, 0, 0, 0, 0]\n",
      "[14, 86, 0, 0, 0, 0] [0.14, 0.86]\n",
      "mse 100 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.14, 0.86] [14, 86, 0, 0, 0, 0]\n",
      "[81, 19, 0, 0, 0, 0] [0.81, 0.19]\n",
      "mse 101 tensor([[0.7041, 0.2959]], grad_fn=<DivBackward0>) [0.81, 0.19] [81, 19, 0, 0, 0, 0]\n",
      "[12, 88, 0, 0, 0, 0] [0.12, 0.88]\n",
      "mse 102 tensor([[0.0186, 0.9814]], grad_fn=<DivBackward0>) [0.12, 0.88] [12, 88, 0, 0, 0, 0]\n",
      "[21, 79, 0, 0, 0, 0] [0.21, 0.79]\n",
      "mse 103 tensor([[0.3936, 0.6064]], grad_fn=<DivBackward0>) [0.21, 0.79] [21, 79, 0, 0, 0, 0]\n",
      "[39, 61, 0, 0, 0, 0] [0.39, 0.61]\n",
      "mse 104 tensor([[0.0012, 0.9988]], grad_fn=<DivBackward0>) [0.39, 0.61] [39, 61, 0, 0, 0, 0]\n",
      "[73, 27, 0, 0, 0, 0] [0.73, 0.27]\n",
      "mse 105 tensor([[0.9195, 0.0805]], grad_fn=<DivBackward0>) [0.73, 0.27] [73, 27, 0, 0, 0, 0]\n",
      "[12, 88, 0, 0, 0, 0] [0.12, 0.88]\n",
      "mse 106 tensor([[0.2381, 0.7619]], grad_fn=<DivBackward0>) [0.12, 0.88] [12, 88, 0, 0, 0, 0]\n",
      "[63, 37, 0, 0, 0, 0] [0.63, 0.37]\n",
      "mse 107 tensor([[0.0435, 0.9565]], grad_fn=<DivBackward0>) [0.63, 0.37] [63, 37, 0, 0, 0, 0]\n",
      "[24, 76, 0, 0, 0, 0] [0.24, 0.76]\n",
      "mse 108 tensor([[0.5494, 0.4506]], grad_fn=<DivBackward0>) [0.24, 0.76] [24, 76, 0, 0, 0, 0]\n",
      "[39, 61, 0, 0, 0, 0] [0.39, 0.61]\n",
      "mse 109 tensor([[0.3534, 0.6466]], grad_fn=<DivBackward0>) [0.39, 0.61] [39, 61, 0, 0, 0, 0]\n",
      "[22, 78, 0, 0, 0, 0] [0.22, 0.78]\n",
      "mse 110 tensor([[0.3340, 0.6660]], grad_fn=<DivBackward0>) [0.22, 0.78] [22, 78, 0, 0, 0, 0]\n",
      "[47, 53, 0, 0, 0, 0] [0.47, 0.53]\n",
      "mse 111 tensor([[0.1892, 0.8108]], grad_fn=<DivBackward0>) [0.47, 0.53] [47, 53, 0, 0, 0, 0]\n",
      "[68, 32, 0, 0, 0, 0] [0.68, 0.32]\n",
      "mse 112 tensor([[0.4962, 0.5038]], grad_fn=<DivBackward0>) [0.68, 0.32] [68, 32, 0, 0, 0, 0]\n",
      "[13, 87, 0, 0, 0, 0] [0.13, 0.87]\n",
      "mse 113 tensor([[0.7484, 0.2516]], grad_fn=<DivBackward0>) [0.13, 0.87] [13, 87, 0, 0, 0, 0]\n",
      "[7, 93, 0, 0, 0, 0] [0.07, 0.93]\n",
      "mse 114 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.07, 0.93] [7, 93, 0, 0, 0, 0]\n",
      "[61, 39, 0, 0, 0, 0] [0.61, 0.39]\n",
      "mse 115 tensor([[0.7628, 0.2372]], grad_fn=<DivBackward0>) [0.61, 0.39] [61, 39, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 116 tensor([[0.6365, 0.3635]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[69, 31, 0, 0, 0, 0] [0.69, 0.31]\n",
      "mse 117 tensor([[0.6914, 0.3086]], grad_fn=<DivBackward0>) [0.69, 0.31] [69, 31, 0, 0, 0, 0]\n",
      "[42, 58, 0, 0, 0, 0] [0.42, 0.58]\n",
      "mse 118 tensor([[0.7685, 0.2315]], grad_fn=<DivBackward0>) [0.42, 0.58] [42, 58, 0, 0, 0, 0]\n",
      "[13, 87, 0, 0, 0, 0] [0.13, 0.87]\n",
      "mse 119 tensor([[0.2351, 0.7649]], grad_fn=<DivBackward0>) [0.13, 0.87] [13, 87, 0, 0, 0, 0]\n",
      "[30, 70, 0, 0, 0, 0] [0.3, 0.7]\n",
      "mse 120 tensor([[0.4287, 0.5713]], grad_fn=<DivBackward0>) [0.3, 0.7] [30, 70, 0, 0, 0, 0]\n",
      "[38, 62, 0, 0, 0, 0] [0.38, 0.62]\n",
      "mse 121 tensor([[0.5768, 0.4232]], grad_fn=<DivBackward0>) [0.38, 0.62] [38, 62, 0, 0, 0, 0]\n",
      "[6, 94, 0, 0, 0, 0] [0.06, 0.94]\n",
      "mse 122 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.06, 0.94] [6, 94, 0, 0, 0, 0]\n",
      "[86, 14, 0, 0, 0, 0] [0.86, 0.14]\n",
      "mse 123 tensor([[0.8161, 0.1839]], grad_fn=<DivBackward0>) [0.86, 0.14] [86, 14, 0, 0, 0, 0]\n",
      "[36, 64, 0, 0, 0, 0] [0.36, 0.64]\n",
      "mse 124 tensor([[0.2771, 0.7229]], grad_fn=<DivBackward0>) [0.36, 0.64] [36, 64, 0, 0, 0, 0]\n",
      "[40, 60, 0, 0, 0, 0] [0.4, 0.6]\n",
      "mse 125 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.4, 0.6] [40, 60, 0, 0, 0, 0]\n",
      "[18, 82, 0, 0, 0, 0] [0.18, 0.82]\n",
      "mse 126 tensor([[0.0974, 0.9026]], grad_fn=<DivBackward0>) [0.18, 0.82] [18, 82, 0, 0, 0, 0]\n",
      "[28, 72, 0, 0, 0, 0] [0.28, 0.72]\n",
      "mse 127 tensor([[0.3805, 0.6195]], grad_fn=<DivBackward0>) [0.28, 0.72] [28, 72, 0, 0, 0, 0]\n",
      "[6, 94, 0, 0, 0, 0] [0.06, 0.94]\n",
      "mse 128 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.06, 0.94] [6, 94, 0, 0, 0, 0]\n",
      "[8, 92, 0, 0, 0, 0] [0.08, 0.92]\n",
      "mse 129 tensor([[0.6309, 0.3691]], grad_fn=<DivBackward0>) [0.08, 0.92] [8, 92, 0, 0, 0, 0]\n",
      "[2, 98, 0, 0, 0, 0] [0.02, 0.98]\n",
      "mse 130 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.02, 0.98] [2, 98, 0, 0, 0, 0]\n",
      "[51, 49, 0, 0, 0, 0] [0.51, 0.49]\n",
      "mse 131 tensor([[0.4899, 0.5101]], grad_fn=<DivBackward0>) [0.51, 0.49] [51, 49, 0, 0, 0, 0]\n",
      "[69, 31, 0, 0, 0, 0] [0.69, 0.31]\n",
      "mse 132 tensor([[0.6188, 0.3812]], grad_fn=<DivBackward0>) [0.69, 0.31] [69, 31, 0, 0, 0, 0]\n",
      "[13, 87, 0, 0, 0, 0] [0.13, 0.87]\n",
      "mse 133 tensor([[0.3539, 0.6461]], grad_fn=<DivBackward0>) [0.13, 0.87] [13, 87, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 134 tensor([[0.9227, 0.0773]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[95, 5, 0, 0, 0, 0] [0.95, 0.05]\n",
      "mse 135 tensor([[0.9562, 0.0438]], grad_fn=<DivBackward0>) [0.95, 0.05] [95, 5, 0, 0, 0, 0]\n",
      "[14, 86, 0, 0, 0, 0] [0.14, 0.86]\n",
      "mse 136 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.14, 0.86] [14, 86, 0, 0, 0, 0]\n",
      "[14, 86, 0, 0, 0, 0] [0.14, 0.86]\n",
      "mse 137 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.14, 0.86] [14, 86, 0, 0, 0, 0]\n",
      "[62, 38, 0, 0, 0, 0] [0.62, 0.38]\n",
      "mse 138 tensor([[0.3230, 0.6770]], grad_fn=<DivBackward0>) [0.62, 0.38] [62, 38, 0, 0, 0, 0]\n",
      "[82, 18, 0, 0, 0, 0] [0.82, 0.18]\n",
      "mse 139 tensor([[0.7990, 0.2010]], grad_fn=<DivBackward0>) [0.82, 0.18] [82, 18, 0, 0, 0, 0]\n",
      "[5, 95, 0, 0, 0, 0] [0.05, 0.95]\n",
      "mse 140 tensor([[0.3089, 0.6911]], grad_fn=<DivBackward0>) [0.05, 0.95] [5, 95, 0, 0, 0, 0]\n",
      "[61, 39, 0, 0, 0, 0] [0.61, 0.39]\n",
      "mse 141 tensor([[0.8300, 0.1700]], grad_fn=<DivBackward0>) [0.61, 0.39] [61, 39, 0, 0, 0, 0]\n",
      "[96, 4, 0, 0, 0, 0] [0.96, 0.04]\n",
      "mse 142 tensor([[0.9735, 0.0265]], grad_fn=<DivBackward0>) [0.96, 0.04] [96, 4, 0, 0, 0, 0]\n",
      "[54, 46, 0, 0, 0, 0] [0.54, 0.46]\n",
      "mse 143 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.54, 0.46] [54, 46, 0, 0, 0, 0]\n",
      "[25, 75, 0, 0, 0, 0] [0.25, 0.75]\n",
      "mse 144 tensor([[0.6541, 0.3459]], grad_fn=<DivBackward0>) [0.25, 0.75] [25, 75, 0, 0, 0, 0]\n",
      "[7, 93, 0, 0, 0, 0] [0.07, 0.93]\n",
      "mse 145 tensor([[0.5885, 0.4115]], grad_fn=<DivBackward0>) [0.07, 0.93] [7, 93, 0, 0, 0, 0]\n",
      "[76, 24, 0, 0, 0, 0] [0.76, 0.24]\n",
      "mse 146 tensor([[0.7316, 0.2684]], grad_fn=<DivBackward0>) [0.76, 0.24] [76, 24, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 147 tensor([[0.9403, 0.0597]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[42, 58, 0, 0, 0, 0] [0.42, 0.58]\n",
      "mse 148 tensor([[0.7518, 0.2482]], grad_fn=<DivBackward0>) [0.42, 0.58] [42, 58, 0, 0, 0, 0]\n",
      "[18, 82, 0, 0, 0, 0] [0.18, 0.82]\n",
      "mse 149 tensor([[0.2436, 0.7564]], grad_fn=<DivBackward0>) [0.18, 0.82] [18, 82, 0, 0, 0, 0]\n",
      "[11, 89, 0, 0, 0, 0] [0.11, 0.89]\n",
      "mse 150 tensor([[0.0786, 0.9214]], grad_fn=<DivBackward0>) [0.11, 0.89] [11, 89, 0, 0, 0, 0]\n",
      "[58, 42, 0, 0, 0, 0] [0.58, 0.42]\n",
      "mse 151 tensor([[0.5439, 0.4561]], grad_fn=<DivBackward0>) [0.58, 0.42] [58, 42, 0, 0, 0, 0]\n",
      "[91, 9, 0, 0, 0, 0] [0.91, 0.09]\n",
      "mse 152 tensor([[0.9266, 0.0734]], grad_fn=<DivBackward0>) [0.91, 0.09] [91, 9, 0, 0, 0, 0]\n",
      "[50, 50, 0, 0, 0, 0] [0.5, 0.5]\n",
      "mse 153 tensor([[0.5572, 0.4428]], grad_fn=<DivBackward0>) [0.5, 0.5] [50, 50, 0, 0, 0, 0]\n",
      "[12, 88, 0, 0, 0, 0] [0.12, 0.88]\n",
      "mse 154 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.12, 0.88] [12, 88, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 155 tensor([[0.8184, 0.1816]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[23, 77, 0, 0, 0, 0] [0.23, 0.77]\n",
      "mse 156 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.23, 0.77] [23, 77, 0, 0, 0, 0]\n",
      "[15, 85, 0, 0, 0, 0] [0.15, 0.85]\n",
      "mse 157 tensor([[0.1614, 0.8386]], grad_fn=<DivBackward0>) [0.15, 0.85] [15, 85, 0, 0, 0, 0]\n",
      "[60, 40, 0, 0, 0, 0] [0.6, 0.4]\n",
      "mse 158 tensor([[0.4833, 0.5167]], grad_fn=<DivBackward0>) [0.6, 0.4] [60, 40, 0, 0, 0, 0]\n",
      "[97, 3, 0, 0, 0, 0] [0.97, 0.03]\n",
      "mse 159 tensor([[0.9858, 0.0142]], grad_fn=<DivBackward0>) [0.97, 0.03] [97, 3, 0, 0, 0, 0]\n",
      "[41, 59, 0, 0, 0, 0] [0.41, 0.59]\n",
      "mse 160 tensor([[0.6268, 0.3732]], grad_fn=<DivBackward0>) [0.41, 0.59] [41, 59, 0, 0, 0, 0]\n",
      "[58, 42, 0, 0, 0, 0] [0.58, 0.42]\n",
      "mse 161 tensor([[0.8722, 0.1278]], grad_fn=<DivBackward0>) [0.58, 0.42] [58, 42, 0, 0, 0, 0]\n",
      "[83, 17, 0, 0, 0, 0] [0.83, 0.17]\n",
      "mse 162 tensor([[0.9248, 0.0752]], grad_fn=<DivBackward0>) [0.83, 0.17] [83, 17, 0, 0, 0, 0]\n",
      "[7, 93, 0, 0, 0, 0] [0.07, 0.93]\n",
      "mse 163 tensor([[0.5247, 0.4753]], grad_fn=<DivBackward0>) [0.07, 0.93] [7, 93, 0, 0, 0, 0]\n",
      "[56, 44, 0, 0, 0, 0] [0.56, 0.44]\n",
      "mse 164 tensor([[0.8899, 0.1101]], grad_fn=<DivBackward0>) [0.56, 0.44] [56, 44, 0, 0, 0, 0]\n",
      "[99, 1, 0, 0, 0, 0] [0.99, 0.01]\n",
      "mse 165 tensor([[0.9976, 0.0024]], grad_fn=<DivBackward0>) [0.99, 0.01] [99, 1, 0, 0, 0, 0]\n",
      "[64, 36, 0, 0, 0, 0] [0.64, 0.36]\n",
      "mse 166 tensor([[0.5418, 0.4582]], grad_fn=<DivBackward0>) [0.64, 0.36] [64, 36, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 167 tensor([[0.9295, 0.0705]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[89, 11, 0, 0, 0, 0] [0.89, 0.11]\n",
      "mse 168 tensor([[0.8699, 0.1301]], grad_fn=<DivBackward0>) [0.89, 0.11] [89, 11, 0, 0, 0, 0]\n",
      "[63, 37, 0, 0, 0, 0] [0.63, 0.37]\n",
      "mse 169 tensor([[0.8703, 0.1297]], grad_fn=<DivBackward0>) [0.63, 0.37] [63, 37, 0, 0, 0, 0]\n",
      "[40, 60, 0, 0, 0, 0] [0.4, 0.6]\n",
      "mse 170 tensor([[0.2275, 0.7725]], grad_fn=<DivBackward0>) [0.4, 0.6] [40, 60, 0, 0, 0, 0]\n",
      "[69, 31, 0, 0, 0, 0] [0.69, 0.31]\n",
      "mse 171 tensor([[0.6134, 0.3866]], grad_fn=<DivBackward0>) [0.69, 0.31] [69, 31, 0, 0, 0, 0]\n",
      "[71, 29, 0, 0, 0, 0] [0.71, 0.29]\n",
      "mse 172 tensor([[0.6634, 0.3366]], grad_fn=<DivBackward0>) [0.71, 0.29] [71, 29, 0, 0, 0, 0]\n",
      "[30, 70, 0, 0, 0, 0] [0.3, 0.7]\n",
      "mse 173 tensor([[0.1791, 0.8209]], grad_fn=<DivBackward0>) [0.3, 0.7] [30, 70, 0, 0, 0, 0]\n",
      "[83, 17, 0, 0, 0, 0] [0.83, 0.17]\n",
      "mse 174 tensor([[0.9231, 0.0769]], grad_fn=<DivBackward0>) [0.83, 0.17] [83, 17, 0, 0, 0, 0]\n",
      "[52, 48, 0, 0, 0, 0] [0.52, 0.48]\n",
      "mse 175 tensor([[0.6336, 0.3664]], grad_fn=<DivBackward0>) [0.52, 0.48] [52, 48, 0, 0, 0, 0]\n",
      "[26, 74, 0, 0, 0, 0] [0.26, 0.74]\n",
      "mse 176 tensor([[0.2681, 0.7319]], grad_fn=<DivBackward0>) [0.26, 0.74] [26, 74, 0, 0, 0, 0]\n",
      "[46, 54, 0, 0, 0, 0] [0.46, 0.54]\n",
      "mse 177 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.46, 0.54] [46, 54, 0, 0, 0, 0]\n",
      "[63, 37, 0, 0, 0, 0] [0.63, 0.37]\n",
      "mse 178 tensor([[0.6989, 0.3011]], grad_fn=<DivBackward0>) [0.63, 0.37] [63, 37, 0, 0, 0, 0]\n",
      "[25, 75, 0, 0, 0, 0] [0.25, 0.75]\n",
      "mse 179 tensor([[0.7739, 0.2261]], grad_fn=<DivBackward0>) [0.25, 0.75] [25, 75, 0, 0, 0, 0]\n",
      "[95, 5, 0, 0, 0, 0] [0.95, 0.05]\n",
      "mse 180 tensor([[0.8131, 0.1869]], grad_fn=<DivBackward0>) [0.95, 0.05] [95, 5, 0, 0, 0, 0]\n",
      "[44, 56, 0, 0, 0, 0] [0.44, 0.56]\n",
      "mse 181 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.44, 0.56] [44, 56, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 182 tensor([[0.9175, 0.0825]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[80, 20, 0, 0, 0, 0] [0.8, 0.2]\n",
      "mse 183 tensor([[0.2017, 0.7983]], grad_fn=<DivBackward0>) [0.8, 0.2] [80, 20, 0, 0, 0, 0]\n",
      "[90, 10, 0, 0, 0, 0] [0.9, 0.1]\n",
      "mse 184 tensor([[0.9198, 0.0802]], grad_fn=<DivBackward0>) [0.9, 0.1] [90, 10, 0, 0, 0, 0]\n",
      "[73, 27, 0, 0, 0, 0] [0.73, 0.27]\n",
      "mse 185 tensor([[0.4369, 0.5631]], grad_fn=<DivBackward0>) [0.73, 0.27] [73, 27, 0, 0, 0, 0]\n",
      "[51, 49, 0, 0, 0, 0] [0.51, 0.49]\n",
      "mse 186 tensor([[0.4265, 0.5735]], grad_fn=<DivBackward0>) [0.51, 0.49] [51, 49, 0, 0, 0, 0]\n",
      "[35, 65, 0, 0, 0, 0] [0.35, 0.65]\n",
      "mse 187 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.35, 0.65] [35, 65, 0, 0, 0, 0]\n",
      "[54, 46, 0, 0, 0, 0] [0.54, 0.46]\n",
      "mse 188 tensor([[0.7715, 0.2285]], grad_fn=<DivBackward0>) [0.54, 0.46] [54, 46, 0, 0, 0, 0]\n",
      "[94, 6, 0, 0, 0, 0] [0.94, 0.06]\n",
      "mse 189 tensor([[0.9103, 0.0897]], grad_fn=<DivBackward0>) [0.94, 0.06] [94, 6, 0, 0, 0, 0]\n",
      "[38, 62, 0, 0, 0, 0] [0.38, 0.62]\n",
      "mse 190 tensor([[0.6942, 0.3058]], grad_fn=<DivBackward0>) [0.38, 0.62] [38, 62, 0, 0, 0, 0]\n",
      "[88, 12, 0, 0, 0, 0] [0.88, 0.12]\n",
      "mse 191 tensor([[0.7497, 0.2503]], grad_fn=<DivBackward0>) [0.88, 0.12] [88, 12, 0, 0, 0, 0]\n",
      "[12, 88, 0, 0, 0, 0] [0.12, 0.88]\n",
      "mse 192 tensor([[0.6013, 0.3987]], grad_fn=<DivBackward0>) [0.12, 0.88] [12, 88, 0, 0, 0, 0]\n",
      "[42, 58, 0, 0, 0, 0] [0.42, 0.58]\n",
      "mse 193 tensor([[0.2142, 0.7858]], grad_fn=<DivBackward0>) [0.42, 0.58] [42, 58, 0, 0, 0, 0]\n",
      "[44, 56, 0, 0, 0, 0] [0.44, 0.56]\n",
      "mse 194 tensor([[0.5547, 0.4453]], grad_fn=<DivBackward0>) [0.44, 0.56] [44, 56, 0, 0, 0, 0]\n",
      "[67, 33, 0, 0, 0, 0] [0.67, 0.33]\n",
      "mse 195 tensor([[0.6542, 0.3458]], grad_fn=<DivBackward0>) [0.67, 0.33] [67, 33, 0, 0, 0, 0]\n",
      "[3, 97, 0, 0, 0, 0] [0.03, 0.97]\n",
      "mse 196 tensor([[0., 1.]], grad_fn=<DivBackward0>) [0.03, 0.97] [3, 97, 0, 0, 0, 0]\n",
      "[8, 92, 0, 0, 0, 0] [0.08, 0.92]\n",
      "mse 197 tensor([[0.6200, 0.3800]], grad_fn=<DivBackward0>) [0.08, 0.92] [8, 92, 0, 0, 0, 0]\n",
      "[93, 7, 0, 0, 0, 0] [0.93, 0.07]\n",
      "mse 198 tensor([[0.9057, 0.0943]], grad_fn=<DivBackward0>) [0.93, 0.07] [93, 7, 0, 0, 0, 0]\n",
      "[33, 67, 0, 0, 0, 0] [0.33, 0.67]\n",
      "mse 199 tensor([[0.3516, 0.6484]], grad_fn=<DivBackward0>) [0.33, 0.67] [33, 67, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sample_listmse = []\n",
    "sample_listmse = train(init_choice = False ,clip_choice=True ,lossfunc_name='mse',sample_list=sample_listmse,celltypechoice = celltypechoice,norm_method_name = 'Min_Max_Scaling')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_norm = ['Min_Max_Scaling','z_score','l2_norm','Max_Scaling']\n",
    "for name in name_norm:\n",
    "    sample_listmae = []\n",
    "    sample_listmse = []\n",
    "    sample_listsmooth = []\n",
    "    sample_list_withoutinit = []\n",
    "    sample_list_withoutclip = []\n",
    "    sample_list_withoutinitandclip = []\n",
    "    sample_listmae = train(init_choice = True ,clip_choice=True ,lossfunc_name='mae',sample_list=sample_listmae,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    sample_listmse = train(init_choice = True ,clip_choice=True ,lossfunc_name='mse',sample_list=sample_listmse,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    sample_listsmooth = train(init_choice = True ,clip_choice=True ,lossfunc_name='smooth',sample_list=sample_listsmooth,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    sample_list_withoutclip = train(init_choice = True ,clip_choice=False ,lossfunc_name='mse',sample_list=sample_list_withoutclip,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    sample_list_withoutinit = train(init_choice = False ,clip_choice=True ,lossfunc_name='mse',sample_list=sample_list_withoutinit,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    sample_list_withoutinitandclip = train(init_choice = False ,clip_choice=False ,lossfunc_name='mse',sample_list=sample_list_withoutinitandclip,celltypechoice = celltypechoice,norm_method_name = name)\n",
    "    ppp = [sample_listmae,\n",
    "    sample_listmse,\n",
    "    sample_listsmooth,\n",
    "    sample_list_withoutinit,\n",
    "    sample_list_withoutclip,\n",
    "    sample_list_withoutinitandclip]\n",
    "    ppp_name = ['mae',\n",
    "    'mse',\n",
    "    'smooth',\n",
    "    'withoutinit',\n",
    "    'withoutclip',\n",
    "    'withoutinitandclip']\n",
    "    for m,n in zip(ppp,ppp_name):\n",
    "        with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-shap/'+name+'-'+n+'.pkl', 'wb') as file:\n",
    "            pickle.dump(m, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_metrics(list1,list2):\n",
    "    x = np.array(list1)\n",
    "    y = np.array(list2)\n",
    "    # 计算均值\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    # 计算标准差\n",
    "    std_x = np.std(x)\n",
    "    std_y = np.std(y)\n",
    "\n",
    "    # 计算皮尔逊相关系数\n",
    "    pearson_corr = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "    # 计算 Lin's CCC\n",
    "    ccc = (2 * pearson_corr * std_x * std_y) / (std_x**2 + std_y**2 + (mean_x - mean_y)**2)\n",
    "\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    m = coefficients[0]\n",
    "    b = coefficients[1]\n",
    "\n",
    "    predicted_y = [m * xx+b for xx in x]\n",
    "    # 计算残差\n",
    "    residuals = [a - b for a, b in zip(y,predicted_y)]\n",
    "    # 计算残差的标准差\n",
    "    residual_std = np.std(residuals)\n",
    "\n",
    "    return pearson_corr,ccc,residual_std,m,b,coefficients\n",
    "def withoutnan(output,target):\n",
    "    i = 0\n",
    "    output_without_nan = []\n",
    "    target_without_nan = []\n",
    "    for x in output:\n",
    "        # if not np.isnan(x) and x>=0 and x<=1:\n",
    "        if not np.isnan(x):\n",
    "            output_without_nan.append(output[i])\n",
    "            target_without_nan.append(target[i])\n",
    "        i = i+1\n",
    "    return output_without_nan,target_without_nan\n",
    "\n",
    "def ListDimConvert(two_dim_list):\n",
    "    one_dim_list = [item for sublist in two_dim_list for item in sublist]\n",
    "    return one_dim_list\n",
    "\n",
    "def plot_point_line(target_list,output_list,savename,loc,save_choice):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    # Example points\n",
    "    x_stick = target_list\n",
    "    y_stick = output_list\n",
    "\n",
    "    # Create a scatter plot for the points\n",
    "    plt.scatter(x_stick, y_stick, c=\"purple\", cmap='viridis',alpha=0.7)\n",
    "    pearson_corr,ccc,residual_std,m,b,coefficients = caculate_metrics(x_stick,y_stick)\n",
    "\n",
    "\n",
    "\n",
    "    #equation_text\n",
    "    all_text = f'y = {m:.3f}x + ({b:.3f})\\nres_std = {residual_std:.4f}\\nPearson\\'s r = {pearson_corr:.4f}\\nLin\\'s ccc = {ccc:.4f}'\n",
    "    equation_text = f'y = {m:.3f}x + ({b:.3f})'\n",
    "    plt.text(-0.4,1.4,equation_text,fontsize = 20,color = 'black')\n",
    "\n",
    "    # res_std = f'res_std = {residual_std:.3f}'\n",
    "    # corr_coeff = f'Pearson\\'s r = {pearson_corr:.3f}'    \n",
    "    # lin_ccc = f'Lin\\'s ccc = {ccc:.3f}'  \n",
    "\n",
    "   # Create the line equation\n",
    "    line = np.poly1d(coefficients)\n",
    "    x_line = np.linspace(min(x_stick), max(x_stick), 100)\n",
    "    y_line = line(x_line)\n",
    "    \n",
    "\n",
    "    # Plot the regression line\n",
    "    # plt.plot(x_line, y_line, linestyle = '--',label=f'Fitted Line (Slope = {m:.2f},residual_std = {residual_std:.2f})', color='y')\n",
    "    plt.plot(x_line, y_line, linestyle = '--', color='y')\n",
    "    \n",
    "    \n",
    "    # plt.text(1.05,0,all_text,fontsize = 15,color = 'black')\n",
    "    plt.axhline(y=1, color='r', linestyle=':')\n",
    "    # 绘制y=0的虚线\n",
    "    plt.axhline(y=0, color='r', linestyle=':')\n",
    "    \n",
    "    # plt.text(0.5,0.13,res_std,fontsize = 15,color = 'b')\n",
    "    # plt.text(0.5,0.06,corr_coeff,fontsize = 15,color = 'b')\n",
    "    # plt.text(0.5,-0.01,lin_ccc,fontsize = 15,color = 'b')\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    # plt.axis('equal')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.xticks(fontsize=15)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('True Fraction',size = 20)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylabel('Predicted Fraction',size = 20)\n",
    "    plt.legend(loc=loc)\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.grid(True)\n",
    "    if save_choice == True:    \n",
    "        plt.savefig(savename,format = 'svg',dpi = 300)\n",
    "    plt.show()\n",
    "    print(all_text)\n",
    "\n",
    "def process_plot(sample_list,savename,save_choice):\n",
    "    output1 = []\n",
    "    output2 = []\n",
    "    # output3 = []\n",
    "    target1 = []\n",
    "    target2 = []\n",
    "    # target3 = []\n",
    "    for j in  range(len(sample_list)):\n",
    "        predict,target_ratio,ratio_num = sample_list[j]\n",
    "        predict1 = predict[:,0].tolist()\n",
    "        predict2 = predict[:,1].tolist()\n",
    "        # predict3 = predict[:,2].tolist()\n",
    "\n",
    "        output1.append(predict1)\n",
    "        output2.append(predict2)\n",
    "        # output3.append(predict3)\n",
    "\n",
    "        target_ratio1 = target_ratio[0]\n",
    "        target_ratio2 = target_ratio[1]\n",
    "        # target_ratio3 = target_ratio[2]\n",
    "\n",
    "        target1.append(target_ratio1)\n",
    "        target2.append(target_ratio2)\n",
    "        # target3.append(target_ratio3)\n",
    "\n",
    "    output1 = ListDimConvert(output1)\n",
    "    output2 = ListDimConvert(output2)\n",
    "    # output3 = ListDimConvert(output3)\n",
    "\n",
    "    output1,target1 = withoutnan(output1,target1)\n",
    "    output2,target2 = withoutnan(output2,target2)\n",
    "    # output3,target3 = withoutnan(output3,target3)\n",
    "    plot_point_line(target1,output1,'/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-new/photosvg/'+'01'+savename,'upper left',save_choice)\n",
    "    # plot_point_line(target2,output2,'/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm/photosvg/'+'02'+savename,'upper left',save_choice)\n",
    "    # plot_point_line(target3,output3,lossfunc+'-'+'135-5.svg','upper left',save_choice)\n",
    "# process_plot(sample_listmse_maxscale,'mse_maxscale',save_choice = False)\n",
    "# process_plot(sample_listmae,'mae',save_choice = False)\n",
    "# process_plot(sample_listmse,'mse',save_choice = False)\n",
    "# process_plot(sample_listsmooth,'smooth',save_choice = False)\n",
    "# process_plot(sample_list_withoutinitandclip,'withoutinitandclip',save_choice = False)\n",
    "# process_plot(sample_list_withoutinit,'withoutinit',save_choice = False)\n",
    "# process_plot(sample_list_withoutclip,'withoutclip',save_choice = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/tempfile-75-minmax/'+'sample_list_withoutclip'+'.pkl', 'wb') as file:\n",
    "#     pickle.dump(sample_list_withoutinitandclip, file)\n",
    "with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-top/z_score-mae.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "process_plot(loaded_data,'l2_norm-withoutclip.svg',save_choice = False)\n",
    "\n",
    "with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-new/z_score-mae.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "process_plot(loaded_data,'Min_Max_Scaling-withoutclip.svg',save_choice = False)\n",
    "\n",
    "with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-shap/z_score-mae.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "process_plot(loaded_data,'Min_Max_Scaling-withoutclip.svg',save_choice = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-new/z_score-withoutclip.pkl', 'rb') as file:\n",
    "#     loaded_data = pickle.load(file)\n",
    "# process_plot(loaded_data,'z_score-withoutclip.svg',save_choice = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建数据\n",
    "#LinCCC\n",
    "# data = np.array([[0.8716, 0.8628, 0.8180],\n",
    "#                  [0.9290, 0.9311, 0.9186],\n",
    "#                  [0.9357, 0.9124, 0.9110]])\n",
    "#STD\n",
    "# data = np.array([[0.1297, 0.1393, 0.1435],\n",
    "#                  [0.1108, 0.1069, 0.1146],\n",
    "#                  [0.0964 , 0.1186, 0.1194]])\n",
    "# P R\n",
    "data = np.array([[0.8814, 0.8706, 0.8247],\n",
    "                 [0.9296, 0.9317, 0.9214],\n",
    "                 [0.9371 , 0.9131, 0.9121]])\n",
    "\n",
    "# 创建热图\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(data, cmap='YlGnBu')  # 使用YlGnBu颜色映射，你可以选择其他颜色映射\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(cax)\n",
    "\n",
    "# 添加横轴和纵轴标签\n",
    "plt.xticks(range(data.shape[1]), ['MAE', 'MSE', 'SMOOTH'])\n",
    "plt.yticks(range(data.shape[0]), ['Min-Max', 'L2-Norm', 'Z-Score'])\n",
    "\n",
    "# 显示数值文本在每个方格中\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        plt.text(j, i, data[i, j], ha='center', va='center', color='black')\n",
    "plt.savefig('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm-new/photosvg/pearson.svg',format = 'svg',dpi = 300)\n",
    "# 显示热图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = [sample_listmae,\n",
    "sample_listmse,\n",
    "sample_listsmooth,\n",
    "sample_list_withoutinit,\n",
    "sample_list_withoutclip,\n",
    "sample_list_withoutinitandclip]\n",
    "\n",
    "\n",
    "ppp_name = ['sample_listmae',\n",
    "'sample_listmse',\n",
    "'sample_listsmooth',\n",
    "'sample_list_withoutinit',\n",
    "'sample_list_withoutclip',\n",
    "'sample_list_withoutinitandclip']\n",
    "\n",
    "\n",
    "for m,n in zip(ppp,ppp_name):\n",
    "    with open('/home/jinzhuo/ww_dataset/DECOMPOSITION/temp-norm/tempfile-75-dataset2-max/'+n+'.pkl', 'wb') as file:\n",
    "        pickle.dump(m, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
